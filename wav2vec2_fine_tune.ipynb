{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyNx3VG+S0HSx7q11pN0MHCf"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/vasudevgupta7/gsoc-wav2vec2@main\n",
        "!pip install pydub\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "lDoifZODlub8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qe2bhCNxh4pW"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "from wav2vec2 import Wav2Vec2Config\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa\n",
        "from datasets import load_metric\n",
        "config = Wav2Vec2Config()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = tfds.load('spoken_digit', split='train', shuffle_files=True)"
      ],
      "metadata": {
        "id": "IvJoemjqqpW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds1=dataset.take(3)\n",
        "for i in ds1:\n",
        "  print(list(i.keys()))\n",
        "  audio = i['audio']\n",
        "  audio_filename = i['audio/filename']\n",
        "  label = i['label']\n",
        "  print(audio.shape, audio_filename, label)\n"
      ],
      "metadata": {
        "id": "YVS0ek7Z8R80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=tfds.as_dataframe(dataset)\n",
        "print(df.shape)\n",
        "df.head()\n",
        "type(df.iloc[:,0])\n",
        "dfx=df['audio']\n",
        "dfy=df['label']\n",
        "dsx = dfx.to_numpy(dfx) \n",
        "dsy = dfy.astype(str)\n",
        "print(len(dsy))"
      ],
      "metadata": {
        "id": "qsYmoOF2EGP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resample(audio):\n",
        "  rs=librosa.resample(audio.astype(np.float64), orig_sr=8000, target_sr=16000)\n",
        "  return rs\n",
        "\n",
        "dsx_iter = map(resample, dsx)\n",
        "dsx=list(dsx_iter)\n",
        "ds = [(dsx[i], dsy[i]) for i in range(len(dsx)) if len(dsx) < AUDIO_MAXLEN]"
      ],
      "metadata": {
        "id": "UWebeuCC9Y8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wav2vec2 import Wav2Vec2Processor\n",
        "tokenizer = Wav2Vec2Processor(is_tokenizer=True)\n",
        "processor = Wav2Vec2Processor(is_tokenizer=False)\n",
        "\n",
        "def preprocess_text(text):\n",
        "  label = tokenizer(text)\n",
        "  return tf.constant(label, dtype=tf.int32)\n",
        "\n",
        "def preprocess_speech(audio):\n",
        "  audio = tf.constant(audio, dtype=tf.float32)\n",
        "  return processor(tf.transpose(audio))\n",
        "\n",
        "def inputs_generator():\n",
        "  for speech, text in ds:\n",
        "    yield preprocess_speech(speech), preprocess_text(text)\n",
        "\n",
        "output_signature = (\n",
        "    tf.TensorSpec(shape=(None),  dtype=tf.float32),\n",
        "    tf.TensorSpec(shape=(None), dtype=tf.int32),\n",
        ")\n",
        "\n",
        "w2v2_ds = tf.data.Dataset.from_generator(inputs_generator, output_signature=output_signature)"
      ],
      "metadata": {
        "id": "L3_RwVewllJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AUDIO_MAXLEN = 246000\n",
        "LABEL_MAXLEN = 256\n",
        "BATCH_SIZE = 2\n",
        "#AUDIO_MAXLEN = 36600\n",
        "#LABEL_MAXLEN = 16\n",
        "#BATCH_SIZE = 1"
      ],
      "metadata": {
        "id": "K-n5k5vRmM_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = len(ds)\n",
        "SEED = 42\n",
        "w2v2_ds = w2v2_ds.shuffle(BUFFER_SIZE, seed=SEED)\n",
        "w2v2_ds = w2v2_ds.padded_batch(BATCH_SIZE, padded_shapes=(AUDIO_MAXLEN, LABEL_MAXLEN), padding_values=(0.0, 0))\n",
        "#w2v2_ds = w2v2_ds.prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "17Zq8AfKl5sp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = w2v2_ds.take(2000)\n",
        "val_ds = w2v2_ds.skip(2000)"
      ],
      "metadata": {
        "id": "NoRf2_BGr-If"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_layer = hub.KerasLayer(\"https://tfhub.dev/vasudevgupta7/wav2vec2/1\", trainable=True)"
      ],
      "metadata": {
        "id": "eBVFF7o4kyBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tf.keras.Input(shape=(AUDIO_MAXLEN,))\n",
        "hidden_states = pretrained_layer(inputs)\n",
        "outputs = tf.keras.layers.Dense(config.vocab_size)(hidden_states)\n",
        "\n",
        "model = tf.keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "owNNcCToiiz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model(tf.random.uniform(shape=(BATCH_SIZE, AUDIO_MAXLEN)))\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "LywUOGPwoHsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wav2vec2 import CTCLoss\n",
        "\n",
        "LEARNING_RATE = 5e-5\n",
        "\n",
        "loss_fn = CTCLoss(config, (BATCH_SIZE, AUDIO_MAXLEN), division_factor=BATCH_SIZE)\n",
        "optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)"
      ],
      "metadata": {
        "id": "F0QoYjBzo79U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer, loss=loss_fn)\n",
        "history = model.fit(train_ds, validation_data=val_ds, epochs=3)\n",
        "history.history"
      ],
      "metadata": {
        "id": "YaAOdcdyqf1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric = load_metric('wer')\n",
        "@tf.function(jit_compile=True)\n",
        "def eval_fwd(batch):\n",
        "  logits = model(batch, training=False)\n",
        "  return tf.argmax(logits, axis=-1)\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "for speech, labels in tqdm(val_ds, total=500):\n",
        "    predictions  = eval_fwd(speech)\n",
        "    predictions = [tokenizer.decode(pred) for pred in predictions.numpy().tolist()]\n",
        "    references = [tokenizer.decode(label, group_tokens=False) for label in labels.numpy().tolist()]\n",
        "    metric.add_batch(references=references, predictions=predictions)\n",
        "metric.compute()"
      ],
      "metadata": {
        "id": "Rp-GTngJ1c1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# finetuned_model = tf.keras.models.load_model(save_dir)"
      ],
      "metadata": {
        "id": "cmS2IzVa3VeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "def extract_characters(batch):\n",
        "  texts = \" \".join(batch[\"text\"])\n",
        "  vocab = list(set(texts))\n",
        "  return {\"vocab\": [vocab], \"texts\": [texts]}\n",
        "\n",
        "vocabs = ds.map(extract_characters, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=ds.column_names[\"train\"])\n",
        "\n",
        "vocab_list = list(set(vocabs[\"train\"][\"vocab\"][0]) | set(vocabs[\"test\"][\"vocab\"][0]))\n",
        "vocab_dict = {v: k for k, v in enumerate(vocab_list)}\n",
        "vocab_dict[\"|\"] = vocab_dict[\" \"]\n",
        "del vocab_dict[\" \"]\n",
        "\n",
        "vocab_dict[\"[UNK]\"] = len(vocab_dict) # add \"unknown\" token \n",
        "vocab_dict[\"[PAD]\"] = len(vocab_dict) # add a padding token that corresponds to CTC's \"blank token\"\n",
        "\n",
        "with open('vocab.json', 'w') as vocab_file:\n",
        "    json.dump(vocab_dict, vocab_file)"
      ],
      "metadata": {
        "id": "zp1rqCIcJd8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# create Wav2Vec2 tokenizer\n",
        "tokenizer = Wav2Vec2CTCTokenizer(\"vocab.json\", unk_token=\"[UNK]\",\n",
        "                                  pad_token=\"[PAD]\", word_delimiter_token=\"|\")\n",
        "\n",
        "# create Wav2Vec2 feature extractor\n",
        "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, \n",
        "                                             padding_value=0.0, do_normalize=True, return_attention_mask=False)\n",
        "# create a processor pipeline \n",
        "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "85vwVQy_3o5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# extract the numerical representation from the dataset\n",
        "def extract_array_samplingrate(batch):\n",
        "    batch[\"speech\"] = batch['audio']['array'].tolist()\n",
        "    batch[\"sampling_rate\"] = batch['audio']['sampling_rate']\n",
        "    batch[\"target_text\"] = batch[\"text\"]\n",
        "    return batch\n",
        "\n",
        "dataset = ds.map(extract_array_samplingrate, remove_columns=ds.column_names[\"train\"])\n",
        "\n",
        "# process the dataset with processor pipeline that created above\n",
        "def process_dataset(batch):  \n",
        "    batch[\"input_values\"] = processor(batch[\"speech\"], \n",
        "                            sampling_rate=batch[\"sampling_rate\"][0]).input_values\n",
        "\n",
        "    with processor.as_target_processor():\n",
        "        batch[\"labels\"] = processor(batch[\"target_text\"]).input_ids\n",
        "    return batch\n",
        "\n",
        "data_processed = dataset.map(process_dataset, \n",
        "                    remove_columns=dataset.column_names[\"train\"], batch_size=8, \n",
        "                    batched=True)\n",
        "\n",
        "train_dataset = data_processed['train']\n",
        "test_dataset = data_processed['test']"
      ],
      "metadata": {
        "id": "dWCDGgcnTASg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}